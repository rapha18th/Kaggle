{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abdhOBYHqYz6",
   "metadata": {},
   "source": [
    "# GRPO Demo\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/tunix/blob/main/examples/grpo_gemma.ipynb) [![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/google/tunix/blob/main/examples/grpo_gemma.ipynb) [![View on GitHub](https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white)](https://github.com/google/tunix/blob/main/examples/grpo_gemma.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce22459e",
   "metadata": {},
   "source": [
    "This tutorial demonstrates training the [Gemma](https://deepmind.google/models/gemma/)\n",
    "3 1B-IT model on the [GSM8K math reasoning benchmark](https://huggingface.co/datasets/openai/gsm8k)\n",
    "using [Group Relative Policy Optimization (GRPO)](https://arxiv.org/pdf/2402.03300).\n",
    "GRPO can enhance your model's problem-solving skills on mathematical word problems,\n",
    "coding problems, etc.\n",
    "\n",
    "GRPO is an RL algorithm designed to enhance the reasoning abilities of LLMs. It\n",
    "is a variant of [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347)\n",
    "that reduces memory usage by eliminating the need for a separate value function\n",
    "model. GRPO works by generating multiple responses for a given prompt,\n",
    "evaluating these responses using a reward model, and then calculating a relative\n",
    "advantage based on the group's performance to update the policy.\n",
    "\n",
    "In this tutorial we use a `v6e-1` TPU for Gemma3-1B-it. Let's get started!\n",
    "\n",
    "Note that the setup below is for the Gemma3-1B-IT model only. If you want to use\n",
    "another model (say, Qwen2.5), you may need to change the setup (for example,\n",
    "tokenizer, chat template, reward function, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y-Xpdbvqwya_",
   "metadata": {},
   "source": [
    "## Install necessary libraries: RESTART AFTER INSTALL FOR COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z03GnyApTn1j",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib.util\n",
    "\n",
    "if importlib.util.find_spec('tensorflow') is None:\n",
    "  print(\"Installing required packages...\")\n",
    "  %pip install -q dotenv\n",
    "  %pip install -q kagglehub\n",
    "  %pip install -q ipywidgets\n",
    "  %pip install -q tensorflow\n",
    "  %pip install -q tensorflow_datasets\n",
    "  %pip install -q tensorboardX\n",
    "  %pip install -q transformers\n",
    "  %pip install -q grain\n",
    "  %pip install -q git+https://github.com/jax-ml/jax\n",
    "  %pip install git+https://github.com/google/tunix\n",
    "  %pip install git+https://github.com/google/qwix\n",
    "  %pip uninstall -q flax -y\n",
    "  %pip install git+https://github.com/google/flax\n",
    "  %pip install -q huggingface_hub\n",
    "  %pip install -q datasets\n",
    "  %pip install -q 'numpy>2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afofSj37qYz6",
   "metadata": {},
   "source": [
    "## Logging into services\n",
    "\n",
    "Also, there is a bug where the code stalls with initializing rl_trainer/grpo_cluster with wandb. Doing this early because it fixes the bug sometimes (but not always - still bugs a lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# Environment & Secrets (Kaggle / Colab safe)\n",
    "# ============================================================\n",
    "# You need access to the Gemma weights on Hugging Face.\n",
    "# On Kaggle: add a Notebook Secret named HF_TOKEN (Settings -> Secrets).\n",
    "# On Colab: add it in Colab Secrets or set env var HF_TOKEN.\n",
    "\n",
    "# Detect Colab\n",
    "try:\n",
    "    from google.colab import userdata  # type: ignore\n",
    "    USE_COLAB = True\n",
    "except Exception:\n",
    "    USE_COLAB = False\n",
    "\n",
    "# Pull token (Colab userdata preferred)\n",
    "if USE_COLAB:\n",
    "    try:\n",
    "        os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\") or os.environ.get(\"HF_TOKEN\", \"\")\n",
    "    except Exception:\n",
    "        os.environ[\"HF_TOKEN\"] = os.environ.get(\"HF_TOKEN\", \"\")\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\").strip()\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise RuntimeError(\n",
    "        \"HF_TOKEN is missing. Add it as an environment variable or Kaggle Secret so the notebook can download Gemma weights.\"\n",
    "    )\n",
    "\n",
    "# Optional: authenticate huggingface_hub (recommended for stable downloads)\n",
    "from huggingface_hub import login\n",
    "login(token=HF_TOKEN, add_to_git_credential=False)\n",
    "\n",
    "print(\"‚úÖ Environment ready. USE_COLAB =\", USE_COLAB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LnF9ZACiTn1k",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "McTNo_r8Tn1k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8f8d0386-6503-4b50-971d-60646bc0c2e7"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from pprint import pprint\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "from huggingface_hub import snapshot_download\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "from pathlib import Path\n",
    "import qwix\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm.auto import tqdm\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma3 import model as gemma_lib\n",
    "from tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n",
    "from tunix.models.gemma3 import params as gemma_params\n",
    "from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.sft import metrics_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Eu_NI9nHTn1k",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Let's define the configuration we are going to use. Note that this is by no\n",
    "means a \"perfect\" set of hyperparameters. To get good results, you might have\n",
    "to train the model for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZPPKme47Tn1k",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Model ======\n",
    "MODEL_ID = \"google/gemma-3-1b-it\"\n",
    "GEMMA_TOKENIZER_PATH = \"gs://gemma-data/tokenizers/tokenizer_gemma3.model\"\n",
    "\n",
    "# ====== Runtime Paths (Kaggle/Colab safe) ======\n",
    "# Kaggle: /kaggle/working is persisted as notebook output\n",
    "# Colab:  /tmp/content is a common writable scratch path\n",
    "WORK_DIR = \"/kaggle/working\" if not USE_COLAB else \"/tmp/content\"\n",
    "CKPT_DIR = os.path.join(WORK_DIR, 'tunix_grpo_ckpts')\n",
    "TB_LOGDIR = os.path.join(WORK_DIR, 'tunix_grpo_logs')\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(TB_LOGDIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ====== Data ======\n",
    "TRAIN_DATA_DIR = \"./data/train\"\n",
    "TEST_DATA_DIR = \"./data/test\"\n",
    "TRAIN_FRACTION = .9\n",
    "\n",
    "# ====== LoRA ======\n",
    "RANK = 64\n",
    "ALPHA = 64.0\n",
    "\n",
    "# ====== Sharding ======\n",
    "# Adjust mesh based on your TPU memory and model size.\n",
    "NUM_TPUS = len(jax.devices())\n",
    "if NUM_TPUS == 8:\n",
    "  MESH_COUNTS = (1, 4)\n",
    "elif NUM_TPUS == 1:\n",
    "  MESH_COUNTS = (1, 1)\n",
    "else:\n",
    "  raise ValueError(f\"Unsupported number of TPUs: {NUM_TPUS}\")\n",
    "\n",
    "MESH = [\n",
    "    MESH_COUNTS,\n",
    "    (\"fsdp\", \"tp\"),\n",
    "]\n",
    "\n",
    "# ====== GRPO ======\n",
    "# === Generation during GRPO training ===\n",
    "MAX_PROMPT_LENGTH = 256\n",
    "TOTAL_GENERATION_STEPS = 768\n",
    "# Important to keep a high-ish temperature for varied, diverse responses during\n",
    "# training.\n",
    "TEMPERATURE = 0.9\n",
    "TOP_P = 1.0\n",
    "TOP_K = 50\n",
    "# The number of times the policy generates multiple responses for a given prompt\n",
    "# within a single training step. This corresponds to `G` in Algorithm 1 in the\n",
    "# paper. The \"group\" in GRPO comes from here.\n",
    "NUM_GENERATIONS = 2\n",
    "\n",
    "# === other GRPO configs ===\n",
    "# The number of iterations per batch (ùúá in GRPO algo 1).\n",
    "NUM_ITERATIONS = 1\n",
    "# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.\n",
    "# Important to keep a high enough value for this, otherwise, the KL divergence\n",
    "# can increase unchecked.\n",
    "BETA = 0.08\n",
    "# Epsilon value for clipping (ùúÄ in GRPO loss in paper). Similar to PPO, for\n",
    "# stable updates.\n",
    "EPSILON = 0.2\n",
    "\n",
    "# ====== Training ======\n",
    "TRAIN_MICRO_BATCH_SIZE = 1\n",
    "# Increase `NUM_BATCHES` and `MAX_STEPS` for better results.\n",
    "NUM_BATCHES = 3738\n",
    "# Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n",
    "# increased to a max. of 330 (if batch size is 4).\n",
    "NUM_TEST_BATCHES = 64\n",
    "\n",
    "EVAL_EVERY_N_STEPS = 64  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\n",
    "NUM_EPOCHS = 1  # can potentially train for more epochs\n",
    "\n",
    "# Number of training steps.\n",
    "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
    "\n",
    "# === AdamW, warmup, cosine scheduler ===\n",
    "LEARNING_RATE = 3e-6\n",
    "B1 = 0.9\n",
    "B2 = 0.99\n",
    "WEIGHT_DECAY = 0.1\n",
    "# == Cosine decay with warmup scheduler ==\n",
    "# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n",
    "# steps, and then gradually decrease the learning rate to 0 using cosine\n",
    "# scheduler.\n",
    "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
    "# == Grad clipping ==\n",
    "# Grad clipping to prevent large gradients. Found this\n",
    "# important to keep KL divergence in check.\n",
    "MAX_GRAD_NORM = 0.1\n",
    "\n",
    "# Checkpoint saving\n",
    "INTERMEDIATE_CKPT_DIR = CKPT_DIR\n",
    "CKPT_DIR = \"/tmp/content/ckpts/\"\n",
    "SAVE_INTERVAL_STEPS = 500\n",
    "MAX_TO_KEEP = 4\n",
    "\n",
    "# ====== Inference ======\n",
    "GENERATION_CONFIGS = {\n",
    "    # greedy search\n",
    "    \"greedy\": {\"temperature\": None, \"top_k\": 1, \"top_p\": None},\n",
    "    # some randomness\n",
    "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    # liberal\n",
    "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ngjtE-63Tn1k",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wjMFOr7aTn1k",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_hbm_usage():\n",
    "  \"\"\"Displays memory usage per device.\"\"\"\n",
    "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
    "\n",
    "  for d in jax.local_devices():\n",
    "    stats = d.memory_stats()\n",
    "    used = stats[\"bytes_in_use\"]\n",
    "    limit = stats[\"bytes_limit\"]\n",
    "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6BtpYMlaTn1k",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "First, let's define some special tokens. We instruct the model to first reason\n",
    "between the `<reasoning>` and `</reasoning>` tokens. After\n",
    "reasoning, we expect it to provide the answer between the `<answer>` and\n",
    "`</answer>` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h6RGv1kSTn1k",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_start = \"<reasoning>\"\n",
    "reasoning_end = \"</reasoning>\"\n",
    "solution_start = \"<answer>\"\n",
    "solution_end = \"</answer>\"\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are given a problem. First, think about the problem \\\n",
    "and provide your reasoning. Place it between {reasoning_start} and \\\n",
    "{reasoning_end}. Then, provide the final answer (i.e., just one numerical \\\n",
    "value) between {solution_start} and {solution_end}.\"\"\"\n",
    "\n",
    "TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{system_prompt}\n",
    "\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WASP9N5JTn1k",
   "metadata": {},
   "source": [
    "We use OpenAI's [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k), which comprises grade school math word problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gTGjcSMNTn1k",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hash_answer(text: str) -> str | None:\n",
    "  if \"####\" not in text:\n",
    "    return None\n",
    "  return text.split(\"####\")[1].strip()\n",
    "\n",
    "\n",
    "def _load_from_tfds(data_dir: str, split: str):\n",
    "  import tensorflow_datasets.text.gsm8k\n",
    "  return tfds.data_source(\n",
    "      \"gsm8k\",\n",
    "      split=split,\n",
    "      data_dir=data_dir,\n",
    "      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
    "      download=True,\n",
    "  )\n",
    "\n",
    "\n",
    "def download_kaggle_dataset(target_dir=\"./data/gsm8k\"):\n",
    "  os.makedirs(target_dir, exist_ok=True)\n",
    "  src = kagglehub.dataset_download(\"thedevastator/grade-school-math-8k-q-a\")\n",
    "  src = Path(src)\n",
    "  dst = Path(target_dir)\n",
    "\n",
    "  for csv_file in src.glob(\"*.csv\"):  # match all CSV files\n",
    "    shutil.copy2(csv_file, dst / csv_file.name)\n",
    "    print(f\"Copied {csv_file.name} ‚Üí {dst/csv_file.name}\")\n",
    "  return target_dir\n",
    "\n",
    "\n",
    "def get_dataset(data_dir, split=\"train\", source=\"tfds\") -> grain.MapDataset:\n",
    "  # Download data\n",
    "  if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "  if source == \"tfds\":\n",
    "    import tensorflow_datasets.text.gsm8k\n",
    "    data = tfds.data_source(\n",
    "        \"gsm8k\",\n",
    "        split=split,\n",
    "        data_dir=data_dir,\n",
    "        builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "  elif source == \"kaggle\":\n",
    "    kaggle_dir = download_kaggle_dataset(data_dir)\n",
    "    file_name = \"main_\" + split + \".csv\"\n",
    "    csv_path = os.path.join(kaggle_dir, file_name)  # adjust filename if needed\n",
    "\n",
    "    data = []\n",
    "    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "      reader = csv.DictReader(csvfile)\n",
    "      for row in reader:\n",
    "        data.append({\n",
    "            \"question\": row[\"question\"],\n",
    "            \"answer\": row[\"answer\"],\n",
    "        })\n",
    "\n",
    "  else:\n",
    "    raise ValueError(f\"Unknown source: {source}\")\n",
    "\n",
    "  def _as_text(v):\n",
    "    return v if isinstance(v, str) else v.decode(\"utf-8\")\n",
    "\n",
    "  dataset = (\n",
    "      grain.MapDataset.source(data)\n",
    "      .shuffle(seed=42)\n",
    "      .map(\n",
    "          lambda x: {\n",
    "              # passed to model forward pass\n",
    "              \"prompts\": TEMPLATE.format(\n",
    "                  system_prompt=SYSTEM_PROMPT,\n",
    "                  question=_as_text(x[\"question\"]),\n",
    "              ),\n",
    "              # passed to reward functions\n",
    "              \"question\": _as_text(x[\"question\"]),\n",
    "              # passed to reward functions\n",
    "              \"answer\": extract_hash_answer(_as_text(x[\"answer\"])),\n",
    "          }\n",
    "      )\n",
    "  )\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uDwobMu_okwv",
   "metadata": {},
   "source": [
    "We split the dataset set into train and test sets as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KXhOL6GyTn1k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ab63b2b9-e317-42aa-df5b-7b08f07db2c5"
   },
   "outputs": [],
   "source": [
    "source = input(\"Choose data source [tfds/kaggle]: \").strip().lower()  # SHOULD I LEAVE THIS OR KEEP IT SET TO KAGGLE\n",
    "\n",
    "if source not in (\"tfds\", \"kaggle\"):\n",
    "  print(\"Invalid choice. Defaulting to 'tfds'.\")\n",
    "  source = \"tfds\"\n",
    "\n",
    "print(f\"Using data source: {source}\")\n",
    "\n",
    "dataset = get_dataset(TRAIN_DATA_DIR, \"train\", source).batch(TRAIN_MICRO_BATCH_SIZE)[\n",
    "    :NUM_BATCHES\n",
    "]\n",
    "\n",
    "if TRAIN_FRACTION == 1.0:\n",
    "  train_dataset = dataset.repeat(NUM_EPOCHS)\n",
    "  val_dataset = None\n",
    "else:\n",
    "  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n",
    "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
    "\n",
    "  val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n",
    "\n",
    "test_dataset = get_dataset(TEST_DATA_DIR, \"test\", source).batch(TRAIN_MICRO_BATCH_SIZE)[\n",
    "    :NUM_TEST_BATCHES\n",
    "]\n",
    "\n",
    "dataset_lengths = (\n",
    "    len(train_dataset),\n",
    "    len(val_dataset) if val_dataset is not None else 0,\n",
    "    len(test_dataset),\n",
    ")\n",
    "print(f\"dataset contains {dataset_lengths} of batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k7n8L0VzTn1k",
   "metadata": {},
   "source": [
    "Let's see how one batch of the training dataset looks like!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5TF-wNQ2Tn1k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "09b4f729-a294-4714-b187-119b1c894811"
   },
   "outputs": [],
   "source": [
    "for ele in train_dataset[:1]:\n",
    "  pprint(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BZxBR7Y_Tn1k",
   "metadata": {},
   "source": [
    "## Load the policy model and the reference model\n",
    "\n",
    "The policy model is the model which is actually trained and whose weights are\n",
    "updated. The reference model is the model with which we compute KL divergence.\n",
    "This is to ensure that the policy updates are not huge and that it does not\n",
    "deviate too much from the reference model.\n",
    "\n",
    "Typically, the reference model is the base model, and the policy model is the\n",
    "same base model, but with LoRA parameters. Only the LoRA parameters are updated.\n",
    "\n",
    "Note: We perform full precision (fp32) training. You can, however, leverage\n",
    "Qwix for QAT.\n",
    "\n",
    "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
    "to have agreed to the Gemma license\n",
    "[here](https://www.kaggle.com/models/google/gemma/flax/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thp6hhqfTn1k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "0b443827f15c4fc38cce2383104e291d",
      "7fa1cfb4299e4d21a82109ded5556d98",
      "7a912e2448c2459282bc01701890cbd6",
      "0c01c559cae74dd19d6709b48bd90c1d",
      "7efae67468de4013ac1ec650bda49b1e",
      "7b6645188d05468ab855c131fe8e1168",
      "fac82407d11e4acaa1ed33e1829c527b",
      "1bc004962d1948dc87ea8f1cea867996",
      "e8e67db5a5314ab889f374e6cd9e4639",
      "795ab503fb7b4081a6e40121e6718094",
      "6eb3b0802d6241a890bc1951237ba40a"
     ]
    },
    "outputId": "4f11488f-5741-4e7d-f447-7093b4dbee46"
   },
   "outputs": [],
   "source": [
    "ignore_patterns = [\n",
    "    \"*.pth\",  # Ignore PyTorch .pth weight files\n",
    "]\n",
    "print(f\"Downloading {MODEL_ID} from Hugging Face...\")\n",
    "local_model_path = snapshot_download(\n",
    "    repo_id=MODEL_ID, ignore_patterns=ignore_patterns\n",
    ")\n",
    "print(f\"Model successfully downloaded to: {local_model_path}\")\n",
    "\n",
    "EOS_TOKENS = []\n",
    "generation_config_path = os.path.join(local_model_path, \"generation_config.json\")\n",
    "if os.path.exists(generation_config_path):\n",
    "  with open(generation_config_path, \"r\") as f:\n",
    "    generation_configs = json.load(f)\n",
    "  EOS_TOKENS = generation_configs.get(\"eos_token_id\", [])\n",
    "  print(f\"Using EOS token IDs: {EOS_TOKENS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nAghcsT_Pmv_",
   "metadata": {},
   "source": [
    "This code snippet serves as a workaround to re-save the pre-trained model checkpoint from Kaggle into a local format that is compatible with the [Flax NNX](https://flax.readthedocs.io/en/stable/why.html) library. Because the original checkpoint has parameter names and tensor structures that don't match the target NNX model architecture, it cannot be loaded directly.\n",
    "\n",
    "We first load the original weights into a temporary model instance, then extract and re-save the model's state into a new, properly formatted local checkpoint, which can then be successfully loaded by the final sharded NNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cIFAxgVOTn1k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "outputId": "0ff77d42-5e28-440d-c72b-0d9ef7adfd26"
   },
   "outputs": [],
   "source": [
    "MODEL_CP_PATH = local_model_path\n",
    "\n",
    "model_config = None\n",
    "if \"gemma-3-270m\" in MODEL_ID:\n",
    "  model_config = gemma_lib.ModelConfig.gemma3_270m()\n",
    "elif \"gemma-3-1b\" in MODEL_ID:\n",
    "  model_config = gemma_lib.ModelConfig.gemma3_1b()\n",
    "else:\n",
    "  raise ValueError(f\"Unknown model id: {MODEL_ID}\")\n",
    "\n",
    "mesh = jax.make_mesh(*MESH, axis_types=(jax.sharding.AxisType.Auto,) * len(MESH[0]))\n",
    "with mesh:\n",
    "  gemma3 = params_safetensors_lib.create_model_from_safe_tensors(\n",
    "      MODEL_CP_PATH, (model_config), mesh\n",
    "  )\n",
    "  nnx.display(gemma3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hpgXONuORkkq",
   "metadata": {},
   "source": [
    "### LoRA Application\n",
    "\n",
    "The `get_lora_model` function takes the base model and applies LoRA layers to it. It uses a `LoraProvider` to select specific layers (like attention and MLP layers) to be adapted. The resulting LoRA-infused model is then sharded and updated to ensure it's ready for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m2KD-nmbTn1k",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lora_model(base_model, mesh):\n",
    "  lora_provider = qwix.LoraProvider(\n",
    "      module_path=(\n",
    "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "          \".*attn_vec_einsum\"\n",
    "      ),\n",
    "      rank=RANK,\n",
    "      alpha=ALPHA,\n",
    "  )\n",
    "\n",
    "  model_input = base_model.get_model_input()\n",
    "  lora_model = qwix.apply_lora_to_model(\n",
    "      base_model, lora_provider, **model_input\n",
    "  )\n",
    "\n",
    "  with mesh:\n",
    "    state = nnx.state(lora_model)\n",
    "    pspecs = nnx.get_partition_spec(state)\n",
    "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "    nnx.update(lora_model, sharded_state)\n",
    "\n",
    "  return lora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mgBALRieR6aY",
   "metadata": {},
   "source": [
    "Now we load reference and policy Gemma models using the Flax NNX library and display their structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4i3CfJ1gTn1k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "outputId": "21bcc280-c851-4caf-9e90-802e07082337"
   },
   "outputs": [],
   "source": [
    "# Policy model\n",
    "lora_policy = get_lora_model(gemma3, mesh=mesh)\n",
    "nnx.display(lora_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4VJTu8LMX2WR",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_lib.Tokenizer(tokenizer_path=GEMMA_TOKENIZER_PATH)\n",
    "if tokenizer.eos_id() not in EOS_TOKENS:\n",
    "  EOS_TOKENS.append(tokenizer.eos_id())\n",
    "  print(f\"Using EOS token IDs: {EOS_TOKENS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zLzR1tJfTn1k",
   "metadata": {},
   "source": [
    "## Define reward functions\n",
    "\n",
    "We define four reward functions:\n",
    "\n",
    "- reward if the format of the output exactly matches the instruction given in\n",
    "`TEMPLATE`;\n",
    "- reward if the format of the output approximately matches the instruction given\n",
    "in `TEMPLATE`;\n",
    "- reward if the answer is correct/partially correct;\n",
    "- Sometimes, the text between `<answer>`, `</answer>` might not be one\n",
    "  number. So, we extract the number, and reward the model if the answer is correct.\n",
    "\n",
    "The reward functions are inspired from\n",
    "[here](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb).\n",
    "\n",
    "First off, let's define a RegEx for checking whether the format matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C7Beft8wTn1k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4db49761-9d6c-41b0-a7bc-d44e1f67491a"
   },
   "outputs": [],
   "source": [
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags=re.MULTILINE | re.DOTALL,\n",
    ")\n",
    "\n",
    "example = f\"{reasoning_start}Let me think!{reasoning_end}{solution_start}2{solution_end}\"\n",
    "print(\"Example:\", example)\n",
    "print(\"Match:\\t\", match_format.search(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fe1rF15zTn1k",
   "metadata": {},
   "source": [
    "Give the model a reward of 3 points if the format matches exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_fhQ6pY2Tn1k",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_exactly(prompts, completions, **kwargs):\n",
    "  return [\n",
    "      0 if match_format.search(response) is None else 3.0\n",
    "      for response in completions\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sWdAdUHuTn1k",
   "metadata": {},
   "source": [
    "We also reward the model if the format of the output matches partially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uOhO4f3-Tn1k",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_approximately(prompts, completions, **kwargs):\n",
    "  scores = []\n",
    "\n",
    "  for completion in completions:\n",
    "    score = 0\n",
    "    response = completion\n",
    "    # Count how many keywords are seen - we penalize if too many!\n",
    "    # If we see 1, then plus some points!\n",
    "    score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
    "    score += 0.5 if response.find(reasoning_start) == 0 else -0.5\n",
    "    score += 0.5 if response.count(reasoning_end) == 1 else -0.5\n",
    "    score += 0.5 if response.count(solution_start) == 1 else -0.5\n",
    "    score += 0.5 if response.count(solution_end) == 1 else -0.5\n",
    "    scores.append(score)\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A2fNZDgTTn1k",
   "metadata": {},
   "source": [
    "Reward the model if the answer is correct. A reward is also given if the answer\n",
    "does not match exactly, i.e., based on how close the answer is to the correct\n",
    "value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S8zcWsmhTn1k",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "  responses = completions\n",
    "\n",
    "  extracted_responses = [\n",
    "      guess.group(1) if r is not None and (guess := match_format.search(r)) is not None else None\n",
    "      for r in responses\n",
    "  ]\n",
    "\n",
    "  scores = []\n",
    "  assert len(extracted_responses) == len(\n",
    "      answer\n",
    "  ), f\"{extracted_responses} and {answer} have mismatching length\"\n",
    "  for guess, true_answer in zip(extracted_responses, answer):\n",
    "    score = 0\n",
    "    if guess is None:\n",
    "      scores.append(0)\n",
    "      continue\n",
    "    # Correct answer gets 3 points!\n",
    "    if guess == true_answer:\n",
    "      score += 3.0\n",
    "    # Match if spaces are seen\n",
    "    elif guess.strip() == true_answer.strip():\n",
    "      score += 1.5\n",
    "    else:\n",
    "      # We also reward it if the answer is close via ratios!\n",
    "      # Ie if the answer is within some range, reward it!\n",
    "      try:\n",
    "        ratio = float(guess) / float(true_answer)\n",
    "        if ratio >= 0.9 and ratio <= 1.1:\n",
    "          score += 0.5\n",
    "        elif ratio >= 0.8 and ratio <= 1.2:\n",
    "          score += 0.25\n",
    "        else:\n",
    "          score -= 1.0  # Penalize wrong answers\n",
    "      except:\n",
    "        score -= 0.5  # Penalize\n",
    "    scores.append(score)\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nIpOVv78Tn1k",
   "metadata": {},
   "source": [
    "Sometimes, the text between `<answer>` and `</answer>` might not be one\n",
    "number; it can be a sentence. So, we extract the number and compare the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NXvRtbk8Tn1k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "88af1421-c762-4009-d1c6-09a47ce52130"
   },
   "outputs": [],
   "source": [
    "match_numbers = re.compile(\n",
    "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "match_numbers.findall(f\"{solution_start}  0.34  {solution_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oxZQAFKOTn1k",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "  question = kwargs[\"question\"]\n",
    "  responses = completions\n",
    "\n",
    "  extracted_responses = [\n",
    "      guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n",
    "      for r in responses\n",
    "  ]\n",
    "\n",
    "  scores = []\n",
    "  print(\"START ============================\")\n",
    "  print(f\"Question:\\t{question[0]}\")\n",
    "  print(f\"Answer:\\t{answer[0]}\")\n",
    "  print(f\"Response:\\t{responses[0]}\")\n",
    "  print(f\"Extracted:\\t{extracted_responses[0]}\")\n",
    "  print(\"END ==============================\")\n",
    "  for guess, true_answer in zip(extracted_responses, answer):\n",
    "    if guess is None:\n",
    "      scores.append(0)\n",
    "      continue\n",
    "    # Convert to numbers\n",
    "    try:\n",
    "      true_answer = float(true_answer.strip())\n",
    "      guess = float(guess.strip())\n",
    "      scores.append(1.5 if guess == true_answer else 0.0)\n",
    "    except:\n",
    "      scores.append(0)\n",
    "      continue\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AaiYMJxFTn1k",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "\n",
    "Before we train the model, let's evaluate the model on the test set so we can\n",
    "see the improvement post training.\n",
    "\n",
    "We evaluate it in two ways:\n",
    "\n",
    "**Quantitative**\n",
    "\n",
    "* **Answer Accuracy**: percentage of samples for which the model predicts the\n",
    "correct final numerical answer  \n",
    "* **Answer (Partial) Accuracy**: percentage of samples for which the model\n",
    "predicts a final numerical answer such that the \\`model answer / answer\\`\n",
    "ratio lies between 0.9 and 1.1.  \n",
    "* **Format Accuracy**: percentage of samples for which the model outputs the\n",
    "correct format, i.e., reasoning between the reasoning special tokens, and the\n",
    "final answer between the \\`\\<start\\_answer\\>\\`, \\`\\<end\\_answer\\>\\` tokens.\n",
    "\n",
    "**Qualitative**\n",
    "\n",
    "We'll also print outputs for a few given questions so that we can compare the generated output later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HAaZ7NjBx99P",
   "metadata": {},
   "source": [
    "We define a helper function to generate an answer, given a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_k58bOicUHJy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None\n",
    "):\n",
    "  \"\"\"Given prompt, generates text.\"\"\"\n",
    "\n",
    "  if isinstance(question, str):\n",
    "    input_batch = [\n",
    "        TEMPLATE.format(\n",
    "            system_prompt=SYSTEM_PROMPT,\n",
    "            question=question,\n",
    "        ),\n",
    "    ]\n",
    "  else:\n",
    "    input_batch = [\n",
    "        TEMPLATE.format(\n",
    "            system_prompt=SYSTEM_PROMPT,\n",
    "            question=q,\n",
    "        )\n",
    "        for q in question\n",
    "    ]\n",
    "\n",
    "  out_data = sampler(\n",
    "      input_strings=input_batch,\n",
    "      max_generation_steps=768,\n",
    "      temperature=temperature,\n",
    "      top_k=top_k,\n",
    "      top_p=top_p,\n",
    "      echo=False,\n",
    "      seed=seed if seed is not None else None,\n",
    "      eos_tokens=EOS_TOKENS,\n",
    "  )\n",
    "\n",
    "  output = out_data.text\n",
    "  if isinstance(question, str):\n",
    "    return output[0]\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zNoa5je7yJOt",
   "metadata": {},
   "source": [
    "Another helper function for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yJo2nuKB-wlw",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    dataset,\n",
    "    sampler,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_passes=1,\n",
    "    corr_lst=False,\n",
    "    make_lst=False,\n",
    "):\n",
    "  \"\"\"Computes accuracy and percentage of outputs matching the format.\"\"\"\n",
    "\n",
    "  response_lst = []\n",
    "  corr = 0\n",
    "  partially_corr = 0\n",
    "  corr_format = 0\n",
    "  total = 0\n",
    "\n",
    "  for batch in tqdm(dataset):\n",
    "    answers = batch[\"answer\"]\n",
    "    questions = batch[\"question\"]\n",
    "\n",
    "    multiple_call_responses = [[] for _ in range(len(questions))]\n",
    "    for p in range(num_passes):\n",
    "      responses = generate(\n",
    "          questions, sampler, temperature, top_k, top_p, seed=p\n",
    "      )\n",
    "      for idx, response in enumerate(responses):\n",
    "        multiple_call_responses[idx].append(response)\n",
    "        print(f\"Question:\\t{questions[idx]}\")\n",
    "        print(f\"Correct Answer:\\t{answers[idx]}\")\n",
    "        print(f\"Response:\\t{response}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    for question, multiple_call_response, answer in zip(\n",
    "        questions, multiple_call_responses, answers\n",
    "    ):\n",
    "      # check answer\n",
    "      corr_ctr_per_question = 0\n",
    "      partially_corr_per_question = 0\n",
    "      corr_format_per_question = 0\n",
    "      for response in multiple_call_response:\n",
    "        extracted_response = (\n",
    "            guess.group(1)\n",
    "            if (guess := match_numbers.search(response)) is not None\n",
    "            else \"-1000000\"\n",
    "        )\n",
    "        try:\n",
    "          if float(extracted_response.strip()) == float(answer.strip()):\n",
    "            corr_ctr_per_question += 1\n",
    "\n",
    "          ratio = float(extracted_response.strip()) / float(answer.strip())\n",
    "          if ratio >= 0.9 and ratio <= 1.1:\n",
    "            partially_corr_per_question += 1\n",
    "        except:\n",
    "          print(\"SKIPPED\")\n",
    "\n",
    "        # check format\n",
    "        if match_format.search(response) is not None:\n",
    "          corr_format_per_question += 1\n",
    "\n",
    "        if (\n",
    "            corr_ctr_per_question > 0\n",
    "            and partially_corr_per_question > 0\n",
    "            and corr_format_per_question > 0\n",
    "        ):\n",
    "          break\n",
    "\n",
    "      if corr_ctr_per_question > 0:\n",
    "        corr += 1\n",
    "        if corr_lst and make_lst:\n",
    "          response_lst.append((question, answer, multiple_call_response))\n",
    "      else:\n",
    "        if not corr_lst and make_lst:\n",
    "          response_lst.append((question, answer, multiple_call_response))\n",
    "      if partially_corr_per_question > 0:\n",
    "        partially_corr += 1\n",
    "      if corr_format_per_question > 0:\n",
    "        corr_format += 1\n",
    "\n",
    "      total += 1\n",
    "      if total % 10 == 0:\n",
    "        print(\n",
    "            f\"===> {corr=}, {total=}, {corr / total * 100=}, \"\n",
    "            f\"{partially_corr / total * 100=}, {corr_format / total * 100=}\"\n",
    "        )\n",
    "\n",
    "  to_return = (\n",
    "      corr,\n",
    "      total,\n",
    "      corr / total * 100,\n",
    "      partially_corr / total * 100,\n",
    "      corr_format / total * 100,\n",
    "  )\n",
    "  if make_lst:\n",
    "    return to_return, response_lst\n",
    "  return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UOAQe06DyVlQ",
   "metadata": {},
   "source": [
    "Now let's see how the original model does on the test set. You can see the percentages of the mode outputs that are fully correct, partially correct and just correct in format. The following step might take couple of minutes to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YQM-tzXWUmoE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d64d92c1de534623abb853a8522650ba",
      "df3424bfda4e41a4aea75e863f95334a",
      "dde8d013ffbb4e51b7b43d5a29df4ac9",
      "b46d988b35d64623a5f694fdae4e2640",
      "ec92058d26c44b89ba45bc82310e128a",
      "827a7875143c4a258971bf2f48ac7f78",
      "411ad58d8efa4592ae6487cb12e6e297",
      "8719751f3cb2482f9c7adcef356d4e7e",
      "0464faee132541899aafa38c79b9b50f",
      "7dad725a201e46a090b4cc084a8bc89e",
      "2278047983234106929f7bf23b51175d"
     ]
    },
    "outputId": "d31acab8-5f98-4455-c3b4-970671bf2288"
   },
   "outputs": [],
   "source": [
    "# The evaluation might take up to couple of minutes to finish.\n",
    "\n",
    "sampler = sampler_lib.Sampler(\n",
    "    transformer=lora_policy,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_config=sampler_lib.CacheConfig(\n",
    "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_kv_heads=model_config.num_kv_heads,\n",
    "        head_dim=model_config.head_dim,\n",
    "    ),\n",
    ")\n",
    "\n",
    "num_correct, total, accuracy, partial_accuracy, format_accuracy = evaluate(\n",
    "    test_dataset,\n",
    "    sampler,\n",
    "    **GENERATION_CONFIGS[\"greedy\"],\n",
    ")\n",
    "print(\n",
    "    f\"{num_correct=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
    "    f\" {format_accuracy=}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-CmB2ZT9Tn1l",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Let's set up all the configs first - checkpointing, metric logging and training.\n",
    "We then train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mHzdsYsGTn1l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ckpt saving\n",
    "checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
    ")\n",
    "\n",
    "# Metrics logger\n",
    "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=TB_LOGDIR, flush_every_n_steps=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9dd597",
   "metadata": {},
   "source": [
    "## (Optional) TensorBoard\n",
    "\n",
    "On **Kaggle**, you can inspect logs by opening the output directory and using Kaggle‚Äôs TensorBoard integration (if available), or by copying the log folder locally.\n",
    "\n",
    "Logs are written to `TB_LOGDIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YWvBkWBsruom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, learning rate scheduler, gradient clipping\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        decay_steps=MAX_STEPS,\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    b1=B1,\n",
    "    b2=B2,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "if MAX_GRAD_NORM is not None:\n",
    "  optimizer = optax.chain(\n",
    "      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
    "      optimizer,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_6VxFW1ZTn1l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    role_to_mesh={\n",
    "        rl_cluster_lib.Role.ACTOR: mesh,\n",
    "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
    "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
    "    },\n",
    "    rollout_engine='vanilla',\n",
    "    offload_to_cpu=False,\n",
    "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
    "        actor_optimizer=optimizer,\n",
    "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "        max_steps=MAX_STEPS,\n",
    "        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
    "        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
    "        # metrics logging\n",
    "        metrics_logging_options=metrics_logging_options,\n",
    "        # checkpoint saving\n",
    "        checkpoint_root_directory=CKPT_DIR,\n",
    "        checkpointing_options=checkpointing_options,\n",
    "    ),\n",
    "    rollout_config=base_rollout.RolloutConfig(\n",
    "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
    "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        top_k=TOP_K,\n",
    "        eos_tokens=EOS_TOKENS,\n",
    "    ),\n",
    ")\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    beta=BETA,\n",
    "    epsilon=EPSILON,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z4yJWiElSmOy",
   "metadata": {},
   "source": [
    "### Setting Up the GRPO Trainer\n",
    "\n",
    "Now we initialize our system for training. First, we create an `RLCluster` instance, which brings together the **policy model (`actor`)**, a **reference model (`reference`)**, and a **tokenizer**. Our `actor` is a trainable LoRA model, while the `reference` is a fixed base model that we use to guide the training.\n",
    "\n",
    "We then create a `GRPOLearner`, the specialized trainer that uses a list of **reward functions** to evaluate and optimize the model's output, completing the RL training setup.\n",
    "\n",
    "Tunix trainers are integrated with [Weights & Biases](https://wandb.ai/) and Tensorboard to help you visualize the training progress. You can choose how you want to use it:\n",
    "\n",
    "**Colab**: Unfortunately, WandB doesn't work nicely with Colab so we disabled it, instead use the tensorboards rendered above to monitor.\n",
    "\n",
    "**Option 1 (Type 1)**: If you're running a quick experiment or just testing things out, choose this. It creates a temporary, private dashboard right in your browser without requiring you to log in or create an account.\n",
    "\n",
    "**Option 2 (Type 2)**: If you have an existing W&B account and want to save your project's history to your personal dashboard, choose this. You'll be prompted to enter your API key or log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OIe1lO08Tn1l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "outputId": "7079fd12-2889-4a43-8e90-bee222191b2d"
   },
   "outputs": [],
   "source": [
    "# RL cluster\n",
    "rl_cluster = rl_cluster_lib.RLCluster(\n",
    "    actor=lora_policy,\n",
    "    reference=gemma3,\n",
    "    tokenizer=tokenizer,\n",
    "    cluster_config=cluster_config,\n",
    ")\n",
    "\n",
    "# GRPO Trainer\n",
    "grpo_trainer = GRPOLearner(\n",
    "    rl_cluster=rl_cluster,\n",
    "    reward_fns=[\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    algo_config=grpo_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b71ed5",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The first couple of training step might take up to 5 minutes to finish. Please be patient. If you experience long training steps, e.g. >10 minutes per step, please open a bug. Really appreciated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S27XDebYTn1l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a8b6d4606df24552a1bcbbfec8769c1a",
      "ee450effebb3424f81302c1c6abff78e",
      "edbd054b89354235a64164a842cbc921",
      "3dd0dd38c9b14fcc9192560fd4a63447",
      "783a50df2dcc4af7a7326224a1f03ace",
      "9e57ab97017a42b3b6dcafaa224db0ec",
      "9e2006311bf14d38aac4dc5d68088bbf",
      "b7b8d21b55934e73bfc4ddcf1b6fa348",
      "9f4315fa70df4fee83918dda04e9fb83",
      "2bbe56ff10ec4951b0512f3f03a465d9",
      "bc8d7d929f9946abafb0bf16ce50a95f"
     ]
    },
    "outputId": "0ef42fc7-42ad-4c2c-c763-e6058134a986"
   },
   "outputs": [],
   "source": [
    "grpo_trainer.train(train_dataset, val_dataset)\n",
    "if not USE_COLAB: wandb.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FzIP8glkTn1l",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "\n",
    "Let's evaluate our finetuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V-73HfP1Tn1l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint first.\n",
    "\n",
    "trained_ckpt_path = os.path.join(\n",
    "    CKPT_DIR, \"actor\", str(MAX_STEPS), \"model_params\"\n",
    ")\n",
    "\n",
    "abs_params = jax.tree.map(\n",
    "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    nnx.state(lora_policy, nnx.LoRAParam),\n",
    ")\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n",
    "\n",
    "nnx.update(\n",
    "    lora_policy,\n",
    "    jax.tree.map(\n",
    "        lambda a, b: b,\n",
    "        nnx.state(lora_policy, nnx.LoRAParam),\n",
    "        trained_lora_params,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nz0q_gGHqYz6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation might take up to couple of minutes to finish. Please be patient.\n",
    "\n",
    "sampler = sampler_lib.Sampler(\n",
    "    transformer=lora_policy,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_config=sampler_lib.CacheConfig(\n",
    "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_kv_heads=model_config.num_kv_heads,\n",
    "        head_dim=model_config.head_dim,\n",
    "    ),\n",
    ")\n",
    "\n",
    "num_correct, total, accuracy, partial_accuracy, format_accuracy = evaluate(\n",
    "    test_dataset,\n",
    "    sampler,\n",
    "    **GENERATION_CONFIGS[\"greedy\"],\n",
    ")\n",
    "print(\n",
    "    f\"{num_correct=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
    "    f\" {format_accuracy=}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1NMAxMh0H5D",
   "metadata": {},
   "source": [
    "With sufficient training, you should see that the percentages of correct model outputs have clearly gone up, which means our training worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GKd9Tp1d0dZS",
   "metadata": {},
   "source": [
    "## Export Merged Lora Weights (Huggingface Format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd899408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Submission Artifacts (non-safetensors ‚úÖ)\n",
    "# ============================================================\n",
    "# The hackathon rules prefer checkpoints loadable by Tunix (not safetensors).\n",
    "# Tunix/Orbax checkpoints are already written during training to CKPT_DIR.\n",
    "#\n",
    "# We will:\n",
    "# 1) Print the latest actor checkpoint path\n",
    "# 2) Zip the full CKPT_DIR folder so you can attach it as a Kaggle Model / output artifact\n",
    "\n",
    "import os, glob, shutil\n",
    "\n",
    "print(\"CKPT_DIR:\", CKPT_DIR)\n",
    "\n",
    "# Find the latest actor step folder\n",
    "actor_dir = os.path.join(CKPT_DIR, \"actor\")\n",
    "steps = []\n",
    "if os.path.isdir(actor_dir):\n",
    "    for name in os.listdir(actor_dir):\n",
    "        if name.isdigit():\n",
    "            steps.append(int(name))\n",
    "steps = sorted(steps)\n",
    "\n",
    "if not steps:\n",
    "    raise RuntimeError(f\"No actor checkpoints found under {actor_dir}. Did training run?\")\n",
    "\n",
    "latest_step = steps[-1]\n",
    "latest_actor_ckpt = os.path.join(actor_dir, str(latest_step))\n",
    "print(\"Latest actor checkpoint step:\", latest_step)\n",
    "print(\"Latest actor checkpoint folder:\", latest_actor_ckpt)\n",
    "\n",
    "# Zip checkpoints (Kaggle will show this in notebook outputs)\n",
    "zip_base = os.path.join(WORK_DIR, f\"{MODEL_ID.replace('/', '_')}_tunix_ckpts\")\n",
    "zip_path = shutil.make_archive(zip_base, 'zip', CKPT_DIR)\n",
    "print(\"‚úÖ Zipped checkpoints to:\", zip_path)\n",
    "\n",
    "print(\"\\nAttach this ZIP (or the CKPT_DIR folder) as your model artifact.\\n\"\n",
    "      \"In your Writeup, mention that this is a Tunix/Orbax checkpoint (not safetensors).\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V6E1",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0464faee132541899aafa38c79b9b50f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0b443827f15c4fc38cce2383104e291d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7fa1cfb4299e4d21a82109ded5556d98",
       "IPY_MODEL_7a912e2448c2459282bc01701890cbd6",
       "IPY_MODEL_0c01c559cae74dd19d6709b48bd90c1d"
      ],
      "layout": "IPY_MODEL_7efae67468de4013ac1ec650bda49b1e"
     }
    },
    "0c01c559cae74dd19d6709b48bd90c1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_795ab503fb7b4081a6e40121e6718094",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6eb3b0802d6241a890bc1951237ba40a",
      "value": "‚Äá10/10‚Äá[00:00&lt;00:00,‚Äá2523.50it/s]"
     }
    },
    "1bc004962d1948dc87ea8f1cea867996": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2278047983234106929f7bf23b51175d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2bbe56ff10ec4951b0512f3f03a465d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3dd0dd38c9b14fcc9192560fd4a63447": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2bbe56ff10ec4951b0512f3f03a465d9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_bc8d7d929f9946abafb0bf16ce50a95f",
      "value": "‚Äá63/3364‚Äá[08:00&lt;50:53,‚Äá‚Äá1.08step/s,‚Äátrain_loss=0.023,‚Äátrain_perplexity=1.02,‚Äátrain_steps_per_sec=17.9,‚Äátrain_kl=0.002]"
     }
    },
    "411ad58d8efa4592ae6487cb12e6e297": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6eb3b0802d6241a890bc1951237ba40a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "783a50df2dcc4af7a7326224a1f03ace": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "795ab503fb7b4081a6e40121e6718094": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a912e2448c2459282bc01701890cbd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1bc004962d1948dc87ea8f1cea867996",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e8e67db5a5314ab889f374e6cd9e4639",
      "value": 10
     }
    },
    "7b6645188d05468ab855c131fe8e1168": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dad725a201e46a090b4cc084a8bc89e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7efae67468de4013ac1ec650bda49b1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fa1cfb4299e4d21a82109ded5556d98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b6645188d05468ab855c131fe8e1168",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_fac82407d11e4acaa1ed33e1829c527b",
      "value": "Fetching‚Äá10‚Äáfiles:‚Äá100%"
     }
    },
    "827a7875143c4a258971bf2f48ac7f78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8719751f3cb2482f9c7adcef356d4e7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e2006311bf14d38aac4dc5d68088bbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e57ab97017a42b3b6dcafaa224db0ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f4315fa70df4fee83918dda04e9fb83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a8b6d4606df24552a1bcbbfec8769c1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ee450effebb3424f81302c1c6abff78e",
       "IPY_MODEL_edbd054b89354235a64164a842cbc921",
       "IPY_MODEL_3dd0dd38c9b14fcc9192560fd4a63447"
      ],
      "layout": "IPY_MODEL_783a50df2dcc4af7a7326224a1f03ace"
     }
    },
    "b46d988b35d64623a5f694fdae4e2640": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7dad725a201e46a090b4cc084a8bc89e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2278047983234106929f7bf23b51175d",
      "value": "‚Äá64/64‚Äá[02:10&lt;00:00,‚Äá‚Äá1.38it/s]"
     }
    },
    "b7b8d21b55934e73bfc4ddcf1b6fa348": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc8d7d929f9946abafb0bf16ce50a95f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d64d92c1de534623abb853a8522650ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df3424bfda4e41a4aea75e863f95334a",
       "IPY_MODEL_dde8d013ffbb4e51b7b43d5a29df4ac9",
       "IPY_MODEL_b46d988b35d64623a5f694fdae4e2640"
      ],
      "layout": "IPY_MODEL_ec92058d26c44b89ba45bc82310e128a"
     }
    },
    "dde8d013ffbb4e51b7b43d5a29df4ac9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8719751f3cb2482f9c7adcef356d4e7e",
      "max": 64,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0464faee132541899aafa38c79b9b50f",
      "value": 64
     }
    },
    "df3424bfda4e41a4aea75e863f95334a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_827a7875143c4a258971bf2f48ac7f78",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_411ad58d8efa4592ae6487cb12e6e297",
      "value": "100%"
     }
    },
    "e8e67db5a5314ab889f374e6cd9e4639": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ec92058d26c44b89ba45bc82310e128a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edbd054b89354235a64164a842cbc921": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7b8d21b55934e73bfc4ddcf1b6fa348",
      "max": 3364,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9f4315fa70df4fee83918dda04e9fb83",
      "value": 63
     }
    },
    "ee450effebb3424f81302c1c6abff78e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e57ab97017a42b3b6dcafaa224db0ec",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9e2006311bf14d38aac4dc5d68088bbf",
      "value": "Actor‚ÄáTraining:‚Äá‚Äá‚Äá2%"
     }
    },
    "fac82407d11e4acaa1ed33e1829c527b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
